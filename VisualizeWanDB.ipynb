{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\"p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# Initialize the WandB API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Set your entity and project\n",
    "entity, project = \"naif\", \"eval_snowclip\"\n",
    "\n",
    "# Define the time range for your runs\n",
    "start_date = \"2024-01-01T00:00:00\"\n",
    "end_date = \"2024-01-31T23:59:59\"\n",
    "\n",
    "# Fetch runs within the specified time range\n",
    "runs = api.runs(entity + \"/\" + project)\n",
    "# Lists to store run data\n",
    "summary_list, config_list, name_list, id_list = [], [], [], []\n",
    "\n",
    "# Iterate over the runs and collect data\n",
    "for run in runs:\n",
    "    # Append summary data, omitting large files\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "    \n",
    "    # Append config data, excluding keys that start with '_'\n",
    "    config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\n",
    "    \n",
    "    # Append the human-readable name of the run\n",
    "    name_list.append(run.name)\n",
    "    \n",
    "    # Append the ID of the run\n",
    "    id_list.append(run.id)\n",
    "\n",
    "# Check if lists are not empty before creating DataFrame\n",
    "if summary_list and config_list and name_list and id_list:\n",
    "    # Create a DataFrame with the collected data\n",
    "    runs_df = pd.DataFrame({\n",
    "        \"summary\": summary_list,\n",
    "        \"config\": config_list,\n",
    "        \"name\": name_list,\n",
    "        \"id\": id_list  # Include the run IDs\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    runs_df.to_csv(\"project.csv\")\n",
    "else:\n",
    "    print(\"No runs found within the specified time range.\")\n",
    "read_csv = pd.read_csv(\"project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary    {'parameters/graph_86image_encoder.CLIP.vision...\n",
       "config     {'loss': {'contrastive_queue_loss': {'temperat...\n",
       "name              GeoCLIP_epoch_4_fold_2_batch_16_queue_2048\n",
       "id                                                  1ant7qdq\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"project.csv\", index_col=0)\n",
    "test['summary']\n",
    "test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_sum_error_distance': 7.555102800619217,\n",
       " 'test_emb_mean_error_distance': 8.37046566821061,\n",
       " 'val_emb_mean_error_distance': 4.32339193609085,\n",
       " 'val_mean_error_distance': 507.37662981771655,\n",
       " 'test_mean_error_distance': 4.53072399675805,\n",
       " 'val_sum_error_distance': 1006.2444047514954}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "big_summary = ast.literal_eval(test.iloc[0].summary)\n",
    "#test.iloc[0].summary[:200]\n",
    "filtered_summary = {\n",
    "    key: value for key, value in big_summary.items() \n",
    "    if \"parameter\" not in key and \"weight\" not in key and \"bias\" not in key and \"gradients\" not in key \n",
    "        and \"distance\"  in key and \"batch\" not in key and \"avg\" not in key\n",
    "\n",
    "}\n",
    "\n",
    "filtered_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['test_mean_error_distance', 'val_batch_mean_error_distance', 'test_batch_mean_error_distance', 'val_sum_error_distance', 'test_avg_error_distance', 'val_avg_error_distance', 'test_batch_sum_error_distance', 'test_emb_mean_error_distance', 'val_batch_sum_error_distance', 'val_emb_mean_error_distance', 'val_mean_error_distance', 'test_sum_error_distance'])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[filtered_summary.keys()]\n",
    "#[filtered_summary.keys()]\n",
    "#[filtered_summary[\"test_correct_city\"]]\n",
    "#[big_summary.keys()]\n",
    "#[filtered_summary[\"test_mean_error_distance\"]]ä\n",
    "#pd.read_csv(\"project.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cwjmtcnw'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'epoch_6.*'\n",
    "read_csv = pd.read_csv(\"project.csv\")\n",
    "name_list = read_csv['name']\n",
    "id_list = read_csv['id']\n",
    "counter = 0\n",
    "\n",
    "id_index_list = []\n",
    "for index, name in enumerate(name_list):\n",
    "    if(re.search(pattern, name)):\n",
    "        counter += 1\n",
    "        id_index_list.append(index)\n",
    "print(counter) #eftersom sista körningen inte kunde köra till epoch 6 så är de bara 14 körningar\n",
    "\n",
    "id_list[id_index_list[0]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.pre_layrnorm.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'val_batch_mean_correct_city_acc', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.bias', 'gradients/graph_82location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.weight', '_timestamp', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.weight', 'gradients/graph_81location_encoder.LocEnc2.head.0.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'parameters/graph_80location_encoder.LocEnc2.head.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.weight', 'gradients/graph_81location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.bias', 'gradients/graph_82location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.weight', 'gradients/graph_82location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_80location_encoder.LocEnc1.capsule.0.b', 'test_sum_error_distance', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_80location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_82image_encoder.CLIP.text_projection.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'gradients/graph_83image_encoder.mlp.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.bias', 'test_batch_sum_correct_street_acc', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'parameters/graph_82location_encoder.LocEnc1.capsule.0.b', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.bias', 'parameters/graph_79location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_83location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.weight', 'gradients/graph_83location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'parameters/graph_82location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'gradients/graph_82image_encoder.mlp.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'parameters/graph_80location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias', 'gradients/graph_80location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_83location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_83location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.weight', 'gradients/graph_82location_encoder.LocEnc1.head.0.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_81location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.pre_layrnorm.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'gradients/graph_82image_encoder.mlp.2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.weight', 'gradients/graph_82location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'gradients/graph_81location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'test_batch_mean_error_distance', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.post_layernorm.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.weight', 'parameters/graph_80location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_82location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.weight', 'parameters/graph_79location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.text_model.final_layer_norm.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.bias', 'gradients/graph_83location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'parameters/graph_83location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.weight', 'gradients/graph_80location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.weight', 'gradients/graph_78location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_81location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.weight', 'gradients/graph_79location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.bias', 'gradients/graph_80location_encoder.LocEnc0.head.0.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_83location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.weight', 'parameters/graph_83location_encoder.LocEnc0.head.0.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'gradients/graph_82logit_scale', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.bias', 'gradients/graph_80image_encoder.mlp.0.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.mlp.2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'test_batch_sum_correct_city_acc', 'gradients/graph_79location_encoder.LocEnc2.capsule.3.weight', 'gradients/graph_79location_encoder.LocEnc2.head.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.bias', 'gradients/graph_80image_encoder.mlp.2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.embeddings.patch_embedding.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_81location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.bias', 'gradients/graph_78location_encoder.LocEnc1.head.0.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'gradients/graph_78location_encoder.LocEnc1.head.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.bias', 'gradients/graph_79location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_78location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.weight', 'parameters/graph_78location_encoder.LocEnc0.head.0.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'test_batch_majority_vote_acc', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'gradients/graph_82location_encoder.LocEnc1.capsule.1.weight', 'test_emb_correct_street', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_projection.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_81location_encoder.LocEnc0.head.0.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.bias', 'gradients/graph_78location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_82image_encoder.mlp.2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.pre_layrnorm.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.weight', 'val_sum_correct_city', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.final_layer_norm.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'gradients/graph_83image_encoder.mlp.2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.embeddings.class_embedding', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.bias', 'train_loss', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.weight', 'parameters/graph_83location_encoder.LocEnc0.capsule.1.bias', 'gradients/graph_83image_encoder.mlp.0.bias', 'parameters/graph_81location_encoder.LocEnc2.head.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.weight', 'gradients/graph_79location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'gradients/graph_82location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_78location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.weight', 'gradients/graph_80logit_scale', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_79location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'gradients/graph_82location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.weight', 'test_avg_error_distance', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'gradients/graph_78location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.post_layernorm.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.bias', 'gradients/graph_79location_encoder.LocEnc2.head.0.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.mlp.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_80location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.weight', 'gradients/graph_80location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_79location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_79location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_83location_encoder.LocEnc2.capsule.1.bias', 'gradients/graph_79location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'val_batch_emb_mean_street_acc', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_80image_encoder.mlp.2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.weight', 'parameters/graph_82location_encoder.LocEnc1.head.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.bias', 'gradients/graph_78location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.final_layer_norm.weight', 'gradients/graph_80location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.weight', 'val_batch_mean_error_distance', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.bias', 'gradients/graph_78location_encoder.LocEnc0.capsule.1.weight', 'gradients/graph_82location_encoder.LocEnc1.head.0.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'gradients/graph_83location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.logit_scale', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.weight', 'gradients/graph_81location_encoder.LocEnc1.head.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.bias', 'parameters/graph_83location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.bias', 'val_sum_correct_street', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.bias', 'parameters/graph_81location_encoder.LocEnc1.head.0.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.bias', 'parameters/graph_78location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'gradients/graph_80location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_82location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'gradients/graph_83location_encoder.LocEnc1.head.0.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_80location_encoder.LocEnc0.head.0.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_82location_encoder.LocEnc2.head.0.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'val_batch_sum_correct_street_acc', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.final_layer_norm.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.bias', 'train_batch_loss', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'gradients/graph_81location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_78image_encoder.mlp.2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'parameters/graph_82location_encoder.LocEnc0.head.0.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.bias', 'val_batch_emb_mean_city_acc', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'parameters/graph_83location_encoder.LocEnc2.head.0.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'test_batch_mean_correct_street_acc', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_79location_encoder.LocEnc2.head.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.bias', 'gradients/graph_80location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.embeddings.patch_embedding.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'gradients/graph_82image_encoder.mlp.2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.weight', 'gradients/graph_81image_encoder.mlp.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'parameters/graph_79location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.bias', 'test_batch_emb_mean_city_acc', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.embeddings.token_embedding.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_82location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.bias', 'gradients/graph_79image_encoder.mlp.2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.embeddings.token_embedding.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_83location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.embeddings.position_embedding.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.bias', 'parameters/graph_80location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_81location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_79image_encoder.mlp.2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.mlp.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.weight', 'gradients/graph_82location_encoder.LocEnc0.head.0.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'gradients/graph_79image_encoder.mlp.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_79location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.weight', 'gradients/graph_80location_encoder.LocEnc1.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.bias', 'gradients/graph_83location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'gradients/graph_81location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias', 'test_loss', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'parameters/graph_80location_encoder.LocEnc1.head.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'gradients/graph_82location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.weight', 'val_emb_correct_city', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_82location_encoder.LocEnc2.capsule.0.b', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.weight', 'gradients/graph_83location_encoder.LocEnc2.head.0.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'test_emb_mean_error_distance', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_81logit_scale', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.bias', 'gradients/graph_79location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_78location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'parameters/graph_80location_encoder.LocEnc2.capsule.0.b', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.post_layernorm.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_82location_encoder.LocEnc1.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'gradients/graph_79image_encoder.mlp.2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'gradients/graph_81location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.bias', 'gradients/graph_83location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_projection.weight', 'parameters/graph_79location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_80location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_81location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_82location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.post_layernorm.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.weight', 'gradients/graph_81location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_80image_encoder.mlp.2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_78location_encoder.LocEnc0.capsule.0.b', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'gradients/graph_80image_encoder.mlp.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.visual_projection.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.weight', 'gradients/graph_83location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_81location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.embeddings.position_embedding.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_81location_encoder.LocEnc0.capsule.0.b', 'parameters/graph_78image_encoder.CLIP.text_model.final_layer_norm.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_projection.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias', 'epoch', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.bias', 'parameters/graph_81image_encoder.mlp.2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_82location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_projection.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'gradients/graph_80location_encoder.LocEnc1.capsule.1.weight', 'val_emb_correct_street', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'gradients/graph_78image_encoder.mlp.2.weight', 'parameters/graph_80location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_81location_encoder.LocEnc1.capsule.0.b', 'gradients/graph_78location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'gradients/graph_83location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_82image_encoder.mlp.2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_83location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.weight', 'gradients/graph_78location_encoder.LocEnc0.head.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.mlp.2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_79location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_83location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.post_layernorm.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.mlp.2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.pre_layrnorm.weight', 'parameters/graph_79image_encoder.CLIP.text_model.embeddings.token_embedding.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_78location_encoder.LocEnc1.capsule.0.b', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.weight', 'gradients/graph_78location_encoder.LocEnc0.head.0.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.bias', 'parameters/graph_80image_encoder.mlp.0.bias', 'parameters/graph_83image_encoder.CLIP.text_model.embeddings.position_embedding.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.bias', 'gradients/graph_81location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.weight', 'parameters/graph_78logit_scale', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.embeddings.position_embedding.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_81location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.embeddings.position_embedding.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.weight', 'gradients/graph_83location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_83location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_81location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_81location_encoder.LocEnc0.head.0.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'gradients/graph_79location_encoder.LocEnc0.head.0.weight', 'parameters/graph_80location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_82location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.logit_scale', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'test_batch_number', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.bias', 'parameters/graph_82image_encoder.mlp.0.bias', 'val_correct_city', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.mlp.0.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_80location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.weight', 'parameters/graph_81location_encoder.LocEnc1.capsule.5.weight', 'gradients/graph_82location_encoder.LocEnc2.head.0.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.mlp.0.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'gradients/graph_79location_encoder.LocEnc0.capsule.3.weight', '_step', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_81image_encoder.mlp.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'gradients/graph_81location_encoder.LocEnc2.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.weight', 'test_iter', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_79location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_81location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_83location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'val_sum_avg_pred', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.bias', 'val_avg_pred', 'parameters/graph_79image_encoder.mlp.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.final_layer_norm.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.embeddings.patch_embedding.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.weight', 'gradients/graph_79location_encoder.LocEnc1.head.0.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_81image_encoder.mlp.2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_79location_encoder.LocEnc0.capsule.0.b', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'gradients/graph_83location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_83location_encoder.LocEnc0.capsule.0.b', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.bias', 'gradients/graph_80location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.embeddings.position_embedding.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.weight', 'parameters/graph_79logit_scale', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_82location_encoder.LocEnc1.capsule.3.bias', 'test_correct_street', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.post_layernorm.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.weight', 'parameters/graph_82location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_82location_encoder.LocEnc0.head.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.pre_layrnorm.weight', 'gradients/graph_81image_encoder.mlp.2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.weight', 'parameters/graph_81location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'gradients/graph_82image_encoder.mlp.0.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.weight', 'parameters/graph_80location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'parameters/graph_83location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.bias', 'val_mean_error_distance', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.weight', 'gradients/graph_80location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.final_layer_norm.bias', 'parameters/graph_78location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'parameters/graph_83location_encoder.LocEnc1.capsule.0.b', 'test_emb_avg_pred', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.weight', 'parameters/graph_82location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_79location_encoder.LocEnc1.capsule.0.b', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_78location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_80location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_79location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.visual_projection.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'parameters/graph_78location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.embeddings.token_embedding.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.weight', 'gradients/graph_82location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.weight', 'gradients/graph_78location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'test_batch_emb_mean_error', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.weight', 'gradients/graph_83location_encoder.LocEnc2.head.0.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_82location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.bias', 'gradients/graph_79location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias', 'gradients/graph_81image_encoder.mlp.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.bias', 'parameters/graph_78image_encoder.mlp.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.weight', 'gradients/graph_81location_encoder.LocEnc0.head.0.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.weight', 'parameters/graph_78location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.embeddings.patch_embedding.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.bias', 'parameters/graph_78location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'gradients/graph_82location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'val_batch_number', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.weight', 'gradients/graph_81image_encoder.mlp.2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_81location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.weight', 'gradients/graph_80location_encoder.LocEnc2.head.0.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.bias', 'gradients/graph_81logit_scale', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.weight', 'gradients/graph_80location_encoder.LocEnc0.head.0.weight', 'gradients/graph_80image_encoder.mlp.2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.weight', 'val_batch_majority_vote_acc', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias', 'gradients/graph_83location_encoder.LocEnc1.head.0.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.weight', 'gradients/graph_78location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.post_layernorm.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'gradients/graph_82location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'val_gps', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.pre_layrnorm.weight', 'gradients/graph_83location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_82image_encoder.mlp.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.bias', 'gradients/graph_79image_encoder.mlp.0.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_80location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'test_batch_loss', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_79location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.weight', 'gradients/graph_83location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_83location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.bias', 'gradients/graph_80location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_81location_encoder.LocEnc2.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.pre_layrnorm.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.post_layernorm.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.mlp.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.pre_layrnorm.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.bias', 'val_correct_street', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'gradients/graph_78image_encoder.mlp.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.embeddings.position_embedding.weight', 'test_sum_correct_city', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_78location_encoder.LocEnc2.head.0.bias', 'parameters/graph_78image_encoder.mlp.2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.final_layer_norm.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.bias', 'parameters/graph_80location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias', 'gradients/graph_79location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.embeddings.class_embedding', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.weight', 'gradients/graph_81location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'gradients/graph_80location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'test_sum_correct_street', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.embeddings.patch_embedding.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_82location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_79image_encoder.mlp.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.embeddings.position_embedding.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.logit_scale', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.post_layernorm.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.weight', 'test_gps', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'parameters/graph_79location_encoder.LocEnc1.head.0.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.bias', 'gradients/graph_78location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'gradients/graph_81location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'gradients/graph_81location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.bias', 'gradients/graph_83logit_scale', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.bias', 'gradients/graph_78location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_78location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.weight', 'val_batch_sum_error_distance', 'parameters/graph_80image_encoder.CLIP.text_model.final_layer_norm.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.bias', 'gradients/graph_79location_encoder.LocEnc1.head.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.weight', 'gradients/graph_83location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.weight', 'gradients/graph_83location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_81location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.embeddings.position_embedding.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'gradients/graph_78location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.logit_scale', 'gradients/graph_78location_encoder.LocEnc2.head.0.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'test_mean_error_distance', 'test_batch_sum_error_distance', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.bias', 'val_batch_mean_correct_street_acc', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_projection.weight', 'test_sum_avg_pred', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.weight', 'gradients/graph_83location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_78location_encoder.LocEnc2.head.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.final_layer_norm.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_82location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.pre_layrnorm.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.bias', 'gradients/graph_81location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_80location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.weight', 'parameters/graph_78location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.weight', 'gradients/graph_79logit_scale', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'gradients/graph_81location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_80location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.weight', 'test_correct_city', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.bias', 'gradients/graph_79location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'parameters/graph_81location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.visual_projection.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.bias', 'gradients/graph_82location_encoder.LocEnc2.capsule.5.weight', 'gradients/graph_79location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_78location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc2.bias', 'parameters/graph_82logit_scale', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.embeddings.position_embedding.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_81location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.weight', 'parameters/graph_79location_encoder.LocEnc2.head.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'gradients/graph_79location_encoder.LocEnc0.head.0.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.bias', 'gradients/graph_82location_encoder.LocEnc2.head.0.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_83location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.weight', 'gradients/graph_83location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.pre_layrnorm.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.logit_scale', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_81location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_80location_encoder.LocEnc0.head.0.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.weight', 'gradients/graph_81location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_79location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_79location_encoder.LocEnc0.head.0.bias', 'gradients/graph_83image_encoder.mlp.2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.bias', 'gradients/graph_82location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_83location_encoder.LocEnc0.head.0.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'parameters/graph_78location_encoder.LocEnc1.head.0.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.weight', 'gradients/graph_78logit_scale', 'parameters/graph_82image_encoder.CLIP.vision_model.embeddings.class_embedding', 'parameters/graph_82location_encoder.LocEnc2.head.0.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_83location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'gradients/graph_83location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_80location_encoder.LocEnc1.capsule.1.weight', 'gradients/graph_81location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc2.bias', 'parameters/graph_80location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_82location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.bias', 'parameters/graph_80location_encoder.LocEnc2.head.0.bias', 'gradients/graph_80location_encoder.LocEnc0.capsule.5.bias', 'gradients/graph_82location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_79location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.weight', 'gradients/graph_81location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.weight', 'gradients/graph_80location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.weight', 'val_emb_avg_pred', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.weight', 'gradients/graph_78image_encoder.mlp.2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'gradients/graph_78location_encoder.LocEnc2.head.0.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.weight', 'gradients/graph_80location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.pre_layrnorm.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.bias', 'gradients/graph_83location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.mlp.fc1.bias', 'gradients/graph_80location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.bias', '_runtime', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc1.weight', 'gradients/graph_81location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'parameters/graph_83location_encoder.LocEnc1.head.0.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.weight', 'gradients/graph_78image_encoder.mlp.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.embeddings.class_embedding', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'gradients/graph_78location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.weight', 'parameters/graph_80logit_scale', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.bias', 'parameters/graph_81location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'test_majority_vote', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'gradients/graph_81location_encoder.LocEnc1.head.0.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_82location_encoder.LocEnc1.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.embeddings.class_embedding', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'parameters/graph_81location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'val_batch_emb_mean_error', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_78location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.final_layer_norm.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_79location_encoder.LocEnc2.capsule.0.b', 'parameters/graph_80image_encoder.CLIP.vision_model.post_layernorm.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_83location_encoder.LocEnc2.capsule.0.b', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'gradients/graph_78location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_83logit_scale', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_80location_encoder.LocEnc2.capsule.5.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.weight', 'parameters/graph_83location_encoder.LocEnc0.capsule.5.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'gradients/graph_78location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_79location_encoder.LocEnc1.head.0.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm1.weight', 'parameters/graph_83location_encoder.LocEnc1.head.0.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'gradients/graph_80location_encoder.LocEnc1.head.0.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.weight', 'gradients/graph_80location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'parameters/graph_83location_encoder.LocEnc2.head.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_79location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'parameters/graph_78location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'val_sum_error_distance', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.post_layernorm.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.weight', 'gradients/graph_83location_encoder.LocEnc0.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.visual_projection.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.visual_projection.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_82location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.embeddings.token_embedding.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.embeddings.position_embedding.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.mlp.fc2.weight', 'gradients/graph_79location_encoder.LocEnc1.capsule.3.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'gradients/graph_78location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.out_proj.weight', 'parameters/graph_78location_encoder.LocEnc1.capsule.3.weight', 'gradients/graph_82location_encoder.LocEnc2.capsule.1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc2.weight', 'val_avg_error_distance', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.weight', 'parameters/graph_78location_encoder.LocEnc2.capsule.3.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'gradients/graph_79location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc1.weight', 'gradients/graph_80location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.embeddings.position_embedding.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.2.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm2.bias', 'test_batch_emb_mean_street_acc', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.bias', 'gradients/graph_78location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc1.weight', 'train_iter', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.9.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.weight', 'parameters/graph_82location_encoder.LocEnc0.capsule.1.bias', 'val_majority_vote', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.logit_scale', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.mlp.fc2.weight', 'gradients/graph_81location_encoder.LocEnc2.capsule.3.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm2.weight', 'gradients/graph_79location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.pre_layrnorm.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.embeddings.patch_embedding.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc1.bias', 'gradients/graph_80location_encoder.LocEnc2.head.0.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'parameters/graph_82location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.weight', 'gradients/graph_82location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.4.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.visual_projection.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.layer_norm1.bias', 'gradients/graph_83location_encoder.LocEnc0.capsule.5.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'gradients/graph_80location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'gradients/graph_78location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'gradients/graph_82location_encoder.LocEnc2.capsule.1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.2.mlp.fc1.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'parameters/graph_81location_encoder.LocEnc2.capsule.0.b', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.9.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_81location_encoder.LocEnc1.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.2.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.6.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.10.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'gradients/graph_78location_encoder.LocEnc2.capsule.3.weight', 'gradients/graph_79location_encoder.LocEnc1.capsule.1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm1.bias', 'val_batch_sum_correct_city_acc', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.8.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.3.self_attn.out_proj.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.5.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'parameters/graph_78location_encoder.LocEnc0.capsule.1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.self_attn.v_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.4.layer_norm1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'parameters/graph_80location_encoder.LocEnc1.head.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.9.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'parameters/graph_78location_encoder.LocEnc0.head.0.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.7.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.bias', 'test_emb_correct_city', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'gradients/graph_81location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.weight', 'parameters/graph_78location_encoder.LocEnc2.capsule.0.b', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.embeddings.class_embedding', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.5.mlp.fc1.bias', 'parameters/graph_78location_encoder.LocEnc1.head.0.bias', 'test_avg_pred', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.embeddings.token_embedding.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.1.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.6.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.self_attn.k_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.10.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.1.mlp.fc2.bias', 'gradients/graph_79location_encoder.LocEnc0.capsule.1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.21.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.1.layer_norm1.bias', 'parameters/graph_80location_encoder.LocEnc0.capsule.0.b', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc2.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.6.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.16.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.19.mlp.fc1.weight', 'gradients/graph_82location_encoder.LocEnc1.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.2.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.layer_norm2.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.4.self_attn.k_proj.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc2.bias', 'parameters/graph_82location_encoder.LocEnc0.capsule.0.b', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.0.self_attn.out_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.4.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.6.layer_norm1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.3.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.9.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.11.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'gradients/graph_79location_encoder.LocEnc1.capsule.1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.14.mlp.fc2.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.1.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.mlp.fc2.weight', 'val_emb_mean_error_distance', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.0.mlp.fc1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.post_layernorm.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.0.layer_norm2.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'parameters/graph_79location_encoder.LocEnc0.capsule.3.weight', 'parameters/graph_79location_encoder.LocEnc0.head.0.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.10.self_attn.q_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_78location_encoder.LocEnc2.capsule.5.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm2.bias', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.weight', 'gradients/graph_81location_encoder.LocEnc0.head.0.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'gradients/graph_83location_encoder.LocEnc0.head.0.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.8.self_attn.out_proj.weight', 'gradients/graph_79location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.9.self_attn.out_proj.bias', 'parameters/graph_79location_encoder.LocEnc0.capsule.3.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc2.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.12.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc1.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.20.layer_norm2.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.0.self_attn.v_proj.bias', 'parameters/graph_82image_encoder.CLIP.text_model.encoder.layers.5.self_attn.k_proj.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.10.layer_norm1.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm2.bias', 'gradients/graph_82location_encoder.LocEnc0.head.0.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.14.layer_norm2.weight', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.mlp.fc1.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.17.mlp.fc2.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.5.mlp.fc2.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.q_proj.bias', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.1.self_attn.v_proj.weight', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.4.self_attn.out_proj.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.8.layer_norm1.bias', 'parameters/graph_79image_encoder.CLIP.text_model.final_layer_norm.weight', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.13.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.10.mlp.fc1.weight', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.23.layer_norm2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.0.mlp.fc1.weight', 'parameters/graph_80image_encoder.CLIP.text_model.encoder.layers.11.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.5.self_attn.v_proj.weight', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.22.mlp.fc2.weight', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.10.mlp.fc2.bias', 'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.18.layer_norm1.bias', 'parameters/graph_80image_encoder.CLIP.vision_model.encoder.layers.3.layer_norm1.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.weight', 'parameters/graph_79location_encoder.LocEnc1.capsule.3.bias', 'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.7.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.11.layer_norm1.weight', 'test_batch_mean_correct_city_acc', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight', 'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias', 'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight', 'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias'])\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "run = wandb.Api().run('naif/eval_snowclip/cwjmtcnw')\n",
    "\n",
    "history_df = run.history(samples=5)\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "history_dict = history_df.to_dict(orient='list')\n",
    "\n",
    "# Now you can print the dictionary keys to see all the metric names\n",
    "print(history_dict.keys())\n",
    "\n",
    "#val_batch_mean_error_distance_data = history_dict['val_batch_mean_error_distance']\n",
    "#print(val_batch_mean_error_distance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.6.layer_norm1.weight',\n",
       "       'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.23.self_attn.out_proj.weight',\n",
       "       'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.7.layer_norm2.weight',\n",
       "       'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.8.mlp.fc1.weight',\n",
       "       'parameters/graph_79image_encoder.CLIP.vision_model.encoder.layers.3.self_attn.v_proj.bias',\n",
       "       'parameters/graph_78image_encoder.CLIP.text_model.encoder.layers.6.mlp.fc2.weight',\n",
       "       'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.2.layer_norm1.bias',\n",
       "       'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       "       'parameters/graph_83image_encoder.CLIP.text_model.encoder.layers.8.layer_norm2.bias',\n",
       "       'parameters/graph_80image_encoder.CLIP.vision_model.pre_layrnorm.bias',\n",
       "       ...\n",
       "       'test_batch_mean_correct_city_acc',\n",
       "       'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.0.self_attn.k_proj.bias',\n",
       "       'parameters/graph_82image_encoder.CLIP.vision_model.encoder.layers.5.layer_norm1.bias',\n",
       "       'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       "       'parameters/graph_81image_encoder.CLIP.text_model.encoder.layers.7.mlp.fc1.weight',\n",
       "       'parameters/graph_79image_encoder.CLIP.text_model.encoder.layers.8.self_attn.v_proj.bias',\n",
       "       'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.16.self_attn.out_proj.bias',\n",
       "       'parameters/graph_81image_encoder.CLIP.vision_model.encoder.layers.19.layer_norm1.bias',\n",
       "       'parameters/graph_78image_encoder.CLIP.vision_model.encoder.layers.15.layer_norm1.weight',\n",
       "       'parameters/graph_83image_encoder.CLIP.vision_model.encoder.layers.8.mlp.fc2.bias'],\n",
       "      dtype='object', length=3968)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_timestamp',\n",
       " 'test_sum_error_distance',\n",
       " 'test_emb_correct_street',\n",
       " 'val_sum_correct_city',\n",
       " 'train_loss',\n",
       " 'test_avg_error_distance',\n",
       " 'val_sum_correct_street',\n",
       " 'test_loss',\n",
       " 'val_emb_correct_city',\n",
       " 'test_emb_mean_error_distance',\n",
       " 'epoch',\n",
       " 'val_emb_correct_street',\n",
       " 'val_correct_city',\n",
       " '_step',\n",
       " 'test_iter',\n",
       " 'val_sum_avg_pred',\n",
       " 'val_avg_pred',\n",
       " 'test_correct_street',\n",
       " 'val_mean_error_distance',\n",
       " 'test_emb_avg_pred',\n",
       " 'val_gps',\n",
       " 'val_correct_street',\n",
       " 'test_sum_correct_city',\n",
       " 'test_sum_correct_street',\n",
       " 'test_gps',\n",
       " 'test_mean_error_distance',\n",
       " 'test_sum_avg_pred',\n",
       " 'test_correct_city',\n",
       " 'val_emb_avg_pred',\n",
       " '_runtime',\n",
       " 'test_majority_vote',\n",
       " 'val_sum_error_distance',\n",
       " 'val_avg_error_distance',\n",
       " 'train_iter',\n",
       " 'val_majority_vote',\n",
       " 'test_emb_correct_city',\n",
       " 'test_avg_pred',\n",
       " 'val_emb_mean_error_distance']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter away parameters and weights and biases from the column names\n",
    "[col for col in history_df.columns if \"parameter\" not in col and \"weight\" not in col and \"bias\" not in col and \"gradients\" not in col and \"batch_\" not in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "test_sum_error_distance\n",
      "[1.3495545531056046, 2.8369867624006053, nan, nan, nan]\n",
      "2522\n",
      "test_sum_correct_city\n",
      "[True, True, None, None, None]\n",
      "2765\n",
      "test_sum_avg_pred\n",
      "[[41.35499572753906, 2.155189037322998], [41.37603759765625, 2.1606645584106445], None, None, None]\n"
     ]
    }
   ],
   "source": [
    "for key, name in enumerate(history_dict):\n",
    "    if \"sum\" in name and history_dict[name][0] :\n",
    "        print(key)\n",
    "        print(name)\n",
    "        print(history_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key, name in enumerate(history_dict):\n",
    "    if \"_mean_error_distance\" in name and history_dict[name][0] :\n",
    "        print(key)\n",
    "        print(name)\n",
    "        print(history_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(index: int, results: pd.DataFrame):\n",
    "    results.to_parquet(f'{id_list[id_index_list[index]]}.parquet')\n",
    "\n",
    "def create_all_run_results(response):\n",
    "    all_run_results = [r for r in response]\n",
    "    results = pd.DataFrame(all_run_results, index=range(len(all_run_results)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoCLIP_epoch_6_fold_0_batch_16_queue_2048\n",
      "5n1qjlak\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(name_list[id_index_list[1]])\n",
    "print(id_list[id_index_list[1]])\n",
    "text = name_list[id_index_list[1]]\n",
    "if \"2048\" in text:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoCLIP_epoch_6_fold_1_batch_16_queue_2048\n",
      "GeoCLIP_epoch_6_fold_0_batch_16_queue_2048\n",
      "GeoCLIP_epoch_6_fold_2_batch_16_queue_1024\n",
      "GeoCLIP_epoch_6_fold_1_batch_16_queue_1024\n",
      "GeoCLIP_epoch_6_fold_0_batch_16_queue_1024\n",
      "GeoCLIP_epoch_6_fold_2_batch_16_queue_512\n",
      "GeoCLIP_epoch_6_fold_1_batch_16_queue_512\n",
      "GeoCLIP_epoch_6_fold_0_batch_16_queue_512\n",
      "GeoCLIP_epoch_6_fold_2_batch_16_queue_256\n",
      "GeoCLIP_epoch_6_fold_1_batch_16_queue_256\n",
      "GeoCLIP_epoch_6_fold_0_batch_16_queue_256\n",
      "GeoCLIP_epoch_6_fold_2_batch_16_queue_128\n",
      "GeoCLIP_epoch_6_fold_1_batch_16_queue_128\n",
      "GeoCLIP_epoch_6_fold_0_batch_16_queue_128\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m id_list\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m)):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name_list[\u001b[43mid_index_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "id_list\n",
    "\n",
    "for i in (range(15)):\n",
    "    print(name_list[id_index_list[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queue_size(run_name: str) -> int:\n",
    "    # Extract the queue size from the run name\n",
    "    queue_size = int(run_name.split('_')[-1])\n",
    "    return queue_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: cwjmtcnw, Queue Size: 2048\n",
      "Run ID: 5n1qjlak, Queue Size: 2048\n",
      "Run ID: 4s6gvjco, Queue Size: 1024\n",
      "Run ID: pdu5mqw1, Queue Size: 1024\n",
      "Run ID: jc5i9wqx, Queue Size: 1024\n",
      "Run ID: wg3v9bki, Queue Size: 512\n",
      "Run ID: qo5ifz28, Queue Size: 512\n",
      "Run ID: 3csm0ha5, Queue Size: 512\n",
      "Run ID: ttumgsg5, Queue Size: 256\n",
      "Run ID: gbxrila5, Queue Size: 256\n",
      "Run ID: 5ux1rtpo, Queue Size: 256\n",
      "Run ID: d7qg2moz, Queue Size: 128\n",
      "Run ID: vsxflmog, Queue Size: 128\n",
      "Run ID: 8qxxesnq, Queue Size: 128\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize the WandB API\n",
    "api = wandb.Api(timeout=40)\n",
    "\n",
    "# Define the keys you want to loop through\n",
    "keys_to_analyze = [\n",
    "    'test_sum_error_distance',\n",
    "    'test_emb_mean_error_distance',\n",
    "    'val_emb_mean_error_distance',\n",
    "    'val_mean_error_distance',\n",
    "    'test_mean_error_distance',\n",
    "    'val_sum_error_distance'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to store the binary values for each run and key\n",
    "binary_values_dict = {key: {128: [], 256: [], 512: [], 1024: [], 2048: []} for key in keys_to_analyze}\n",
    "means_dict = {}\n",
    "std_devs_dict = {}\n",
    "\n",
    "# Loop through all the runs you want\n",
    "for i in range(len(id_index_list)):\n",
    "    run_id = id_list[id_index_list[i]]\n",
    "    run_name = name_list[id_index_list[i]]\n",
    "    queue_size = extract_queue_size(run_name)\n",
    "    print(f'Run ID: {run_id}, Queue Size: {queue_size}')\n",
    "    run = api.run(f'naif/eval_snowclip/{run_id}')\n",
    "\n",
    "    # New outer loop to go through each key\n",
    "    for key in keys_to_analyze:\n",
    "        history = run.scan_history(keys=[key])\n",
    "\n",
    "        # Convert values to binary based on the condition\n",
    "        binary_values = [0 if item[key] > 25 else 1 for item in history]\n",
    "\n",
    "        # Add the binary values for this run to the list\n",
    "        binary_values_dict[key][queue_size].extend(binary_values)\n",
    "\n",
    "# After collecting all the binary values\n",
    "for key, queue_dict in binary_values_dict.items():\n",
    "    for queue_size, binary_values in queue_dict.items():\n",
    "        if binary_values:\n",
    "            # Convert the binary values list to a pandas DataFrame\n",
    "            df = pd.DataFrame(binary_values, columns=['binary_value'])\n",
    "            # Save the DataFrame to a Parquet file\n",
    "            df.to_parquet(f'{key}_queue_size_{queue_size}.parquet')\n",
    "\n",
    "    # Plot the mean and standard deviation for 'test_sum_error_distance' to verify the data\n",
    "    if key == 'test_sum_error_distance':\n",
    "        for queue_size, values in queue_dict.items():\n",
    "            if values:  # Check if the list is not empty\n",
    "                mean_binary = np.mean(values)\n",
    "                std_binary = np.std(values)\n",
    "                std_binary = std_binary/np.sqrt(len(values))  # Standard error of the mean\n",
    "                \n",
    "                means_dict[queue_size] = mean_binary\n",
    "                std_devs_dict[queue_size] = std_binary\n",
    "\n",
    "        # Plotting the mean with standard deviation\n",
    "        queue_sizes = sorted(means_dict.keys())\n",
    "        means = [means_dict[q] for q in queue_sizes]\n",
    "        std_devs = [std_devs_dict[q] for q in queue_sizes]\n",
    "\n",
    " \n",
    "\n",
    "def extract_queue_size(run_name: str) -> int:\n",
    "    # Extract the queue size from the run name\n",
    "    queue_size = int(run_name.split('_')[-1])\n",
    "    return queue_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mApi(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaif/eval_snowclip/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_list[\u001b[43mid_index_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m  )\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39mscan_history()\n\u001b[0;32m      8\u001b[0m results \u001b[38;5;241m=\u001b[39m create_all_run_results(response)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#TODO make a loop that goes through id_list to get all runs\n",
    "\n",
    "for i in id_index_list:\n",
    "    if i < 2:\n",
    "        continue\n",
    "    run = wandb.Api(timeout=40).run(f'naif/eval_snowclip/{id_list[id_index_list[i]]}'  )\n",
    "    response = run.scan_history()\n",
    "    results = create_all_run_results(response)\n",
    "    write_parquet(i, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.Api(timeout=40).run(f'naif/eval_snowclip/{id_list[id_index_list[3]]}')\n",
    "response = run.scan_history()\n",
    "#results = create_all_run_results(response)\n",
    "#write_parquet(i, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_run_results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#results = pd.DataFrame(all_run_results, index=range(len(all_run_results)))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\apis\\public\\history.py:50\u001b[0m, in \u001b[0;36mHistoryScan.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_offset \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_step:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\apis\\normalize.py:41\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhoa, you found a bug.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     43\u001b[0m     errors \u001b[38;5;241m=\u001b[39m parse_backend_error_messages(error\u001b[38;5;241m.\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\retry.py:212\u001b[0m, in \u001b[0;36mretriable.<locals>.decorator.<locals>.wrapped_fn\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\retry.py:131\u001b[0m, in \u001b[0;36mRetry.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;66;03m# Only print resolved attempts once every minute\u001b[39;00m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m now \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_print \u001b[38;5;241m>\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(\n\u001b[0;32m    134\u001b[0m             minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    135\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\apis\\public\\history.py:72\u001b[0m, in \u001b[0;36mHistoryScan._load_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m     max_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_step\n\u001b[0;32m     63\u001b[0m variables \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mentity,\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mproject,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpageSize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_size),\n\u001b[0;32m     70\u001b[0m }\n\u001b[1;32m---> 72\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrows \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m res]\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\retry.py:212\u001b[0m, in \u001b[0;36mretriable.<locals>.decorator.<locals>.wrapped_fn\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\retry.py:131\u001b[0m, in \u001b[0;36mRetry.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;66;03m# Only print resolved attempts once every minute\u001b[39;00m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m now \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_print \u001b[38;5;241m>\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(\n\u001b[0;32m    134\u001b[0m             minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    135\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\apis\\public\\api.py:73\u001b[0m, in \u001b[0;36mRetryingClient.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;129m@retry\u001b[39m\u001b[38;5;241m.\u001b[39mretriable(\n\u001b[0;32m     67\u001b[0m     retry_timedelta\u001b[38;5;241m=\u001b[39mRETRY_TIMEDELTA,\n\u001b[0;32m     68\u001b[0m     check_retry_fn\u001b[38;5;241m=\u001b[39mutil\u001b[38;5;241m.\u001b[39mno_retry_auth,\n\u001b[0;32m     69\u001b[0m     retryable_exceptions\u001b[38;5;241m=\u001b[39m(RetryError, requests\u001b[38;5;241m.\u001b[39mRequestException),\n\u001b[0;32m     70\u001b[0m )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mReadTimeout:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py:52\u001b[0m, in \u001b[0;36mClient.execute\u001b[1;34m(self, document, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(document)\n\u001b[1;32m---> 52\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merrors:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(result\u001b[38;5;241m.\u001b[39merrors[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py:60\u001b[0m, in \u001b[0;36mClient._get_result\u001b[1;34m(self, document, *args, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, document, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries:\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     last_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     retries_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\lib\\gql_request.py:58\u001b[0m, in \u001b[0;36mGraphQLSession.execute\u001b[1;34m(self, document, variable_values, timeout)\u001b[0m\n\u001b[0;32m     51\u001b[0m data_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_json \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m post_args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcookies\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies,\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_timeout,\n\u001b[0;32m     56\u001b[0m     data_key: payload,\n\u001b[0;32m     57\u001b[0m }\n\u001b[1;32m---> 58\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpost_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m request\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     61\u001b[0m result \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1253\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1252\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "all_run_results = [r for r in response]\n",
    "#results = pd.DataFrame(all_run_results, index=range(len(all_run_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_run_results, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_run_results)))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3101\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3022\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3098\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3099\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:476\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    475\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 476\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m    480\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    481\u001b[0m     df,\n\u001b[0;32m    482\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Naifh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     65\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot_key_visualization(key, queue_sizes, means, std_devs):\n",
    "    plt.errorbar(queue_sizes, means, yerr=std_devs, fmt='-o')\n",
    "    plt.fill_between(queue_sizes, np.array(means) - np.array(std_devs), np.array(means) + np.array(std_devs), color='gray', alpha=0.2)\n",
    "    plt.title(f'{key} - Mean and Standard Error per Queue Size')\n",
    "    plt.xlabel('Queue Size')\n",
    "    plt.ylabel('Binary Value Mean')\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    # Set the x-axis to show specific queue sizes\n",
    "    plt.xticks(queue_sizes, labels=queue_sizes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    file_path = os.path.join(save_directory, f'{key}_visualization.png')\n",
    "    plt.savefig(file_path)  # Save the plot as a PNG file\n",
    "    print(file_path)\n",
    "    plt.clf()  # Clear the current figure for the next plot\n",
    "    plt.close()\n",
    "\n",
    "binary_values_dict = {key: {128: [], 256: [], 512: [], 1024: [], 2048: []} for key in keys_to_analyze}\n",
    "means_dict = {}\n",
    "std_devs_dict = {}\n",
    "save_directory = r'C:\\Users\\Naifh\\code\\viz_data'\n",
    "\n",
    "parquet_directory = r'C:\\Users\\Naifh\\code\\viz_data'\n",
    "for filename in os.listdir(parquet_directory):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        key = filename.split('_queue_size')[0]  # Extract the key from the filename\n",
    "        queue_size = int(filename.split('_queue_size_')[1].split('.parquet')[0])  # Extract the queue size from the filename\n",
    "        \n",
    "        df = pd.read_parquet(os.path.join(parquet_directory, filename))\n",
    "\n",
    "        mean = df['binary_value'].mean()\n",
    "        std_binary = df['binary_value'].std() / np.sqrt(len(df))\n",
    "\n",
    "\n",
    "        if key not in means_dict:\n",
    "            means_dict[key] = {}\n",
    "            std_devs_dict[key] = {}\n",
    "        print(mean)\n",
    "        means_dict[key][queue_size] = mean\n",
    "        std_devs_dict[key][queue_size] = std_binary\n",
    "\n",
    "for keys in means_dict:\n",
    "\n",
    "    queue_sizes = sorted(means_dict[keys].keys())\n",
    "    means = [means_dict[keys][q] for q in queue_sizes]\n",
    "    std_devs = [std_devs_dict[keys][q] for q in queue_sizes]\n",
    "    plot_key_visualization(keys, queue_sizes, means, std_devs)\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
