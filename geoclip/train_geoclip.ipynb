{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GeoCLIP\n",
    "from model import ImageEncoder\n",
    "from model import LocationEncoder\n",
    "from train import train\n",
    "from train import dataloader\n",
    "import os\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float()\n",
    "\n",
    "\n",
    "    def calc_similarity_batch(self, a, b):\n",
    "       representations = torch.cat([a, b], dim=0)\n",
    "       return F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "\n",
    "        \n",
    "    def forward(self, proj_1, proj_2):\n",
    "        \"\"\"\n",
    "        proj_1 and proj_2 are batched embeddings [batch, embedding_dim]\n",
    "        where corresponding indices are pairs\n",
    "        z_i, z_j in the SimCLR paper\n",
    "        \"\"\"\n",
    "        batch_size = proj_1.shape[0]\n",
    "        z_i = F.normalize(proj_1, p=2, dim=1)\n",
    "        z_j = F.normalize(proj_2, p=2, dim=1)\n",
    "\n",
    "        similarity_matrix = self.calc_similarity_batch(z_i, z_j)\n",
    "\n",
    "        # Postive similarities are on the off-diagonals\n",
    "        sim_ij = torch.diag(similarity_matrix, batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -batch_size)\n",
    "\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "\n",
    "        # Mask out the main diagonal and calculate the softmax\n",
    "        denominator = (self.mask, similarity_matrix) * torch.exp(similarity_matrix / self.temperature)\n",
    "\n",
    "        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "model.safetensors: 100%|██████████| 1.71G/1.71G [02:39<00:00, 10.7MB/s]\n",
      "preprocessor_config.json: 100%|██████████| 316/316 [00:00<00:00, 1.58MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 905/905 [00:00<00:00, 4.24MB/s]\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "vocab.json: 100%|██████████| 961k/961k [00:00<00:00, 2.05MB/s]\n",
      "merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 11.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.22M/2.22M [00:00<00:00, 3.44MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 389/389 [00:00<00:00, 2.17MB/s]\n"
     ]
    }
   ],
   "source": [
    "#loader = dataloader.GeoDataLoader(\"data/geojsons\", \"data/images\")\n",
    "geo_clip = GeoCLIP()\n",
    "#train(train_dataloader=loader, model=geo_clip)\n",
    "\n",
    "#geo_clip.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  22.1980, -159.6219],\n",
       "         [  22.1785, -159.6501],\n",
       "         [  22.1759, -159.6542],\n",
       "         [  22.1751, -159.6559],\n",
       "         [  22.1502, -159.6636],\n",
       "         [  22.2178, -159.5888],\n",
       "         [  21.4986, -158.1512],\n",
       "         [  22.2207, -159.5827],\n",
       "         [  21.3382, -157.8054],\n",
       "         [  20.1172, -155.5842]]),\n",
       " tensor([0.0731, 0.0704, 0.0679, 0.0669, 0.0527, 0.0454, 0.0370, 0.0314, 0.0293,\n",
       "         0.0288]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_clip.predict(\"/workspace/geoclip/images/Kauai.png\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(geo_clip.parameters(), lr=0.1)\n",
    "criterion = SimCLRLoss(10, 0.1)\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\u001b[43mtrain_dataloader\u001b[49m, geo_clip, criterion, optim, scheduler, epoch \u001b[38;5;241m=\u001b[39m EPOCHS, batch_size \u001b[38;5;241m=\u001b[39mBATCH_SIZE, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, geo_clip, criterion, optim, scheduler, epoch = EPOCHS, batch_size =BATCH_SIZE, device = \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
